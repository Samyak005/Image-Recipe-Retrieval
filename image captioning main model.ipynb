{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(captions_filename, features_filename):\n",
    "  features = pd.read_csv(features_filename, sep=',',header=None)\n",
    "  texts = []\n",
    "  with open(captions_filename) as fp:\n",
    "    for line in fp:\n",
    "      tokens = line.strip().split()\n",
    "      texts.append(' '.join(tokens[1:]))\n",
    "  return features, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curly Q Noodle Chicken Soup\n"
     ]
    }
   ],
   "source": [
    "features, texts = load('../input/train-1000-recipe-images/rec_imgID_train_1000.txt', '../input/train-1000-recipe-images/train_image_encoding1000.csv')\n",
    "print(texts[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Making Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_texts = []\n",
    "with open('../input/train-1000-recipe-images/rec_imgID_train_1000.txt') as fp:\n",
    "    for line in fp:\n",
    "      tokens = line.strip().split()\n",
    "      vocab_texts.append(' '.join(tokens[1:]))\n",
    "        \n",
    "for i in range(1999,24000,1000):\n",
    "    with open('../input/train-labels/train_label_' + str(i) + '.txt') as fp:\n",
    "        for line in fp:\n",
    "          tokens = line.strip().split()\n",
    "          vocab_texts.append(' '.join(tokens[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pizza 52\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(vocab_texts)\n",
    "\n",
    "vocab = tokenizer.word_index\n",
    "vocab['<eos>'] = 0 # add word with id 0\n",
    "print('pizza', vocab['pizza'])\n",
    "import json\n",
    "with open('vocab.json', 'w') as fp: # save the vocab\n",
    "    fp.write(json.dumps(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...   22    3 2276]\n",
      " [   0    0    0 ...  298 1582    8]\n",
      " [   0    0    0 ...    0  140  809]\n",
      " ...\n",
      " [   0    0    0 ...  956  130    8]\n",
      " [   0    0    0 ...    1 1787   33]\n",
      " [   0    0    0 ...  232    6    9]]\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "captions = pad_sequences(sequences, maxlen=16)\n",
    "print(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading embeddings from \"../input/glove840b300dtxt/glove.840B.300d.txt\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pizza [ 0.0068727 -0.21634    0.27831   -0.26192    0.22884    0.89332\n",
      "  0.4131     0.27377    0.22652    1.5041    -0.58059    0.56083\n",
      " -0.18432    0.27738   -0.10709   -0.13519    0.023817   1.1765\n",
      " -0.12659    0.043173 ]\n"
     ]
    }
   ],
   "source": [
    "import fork_of_embedding_loader_keras as embedding\n",
    "embedding_weights = embedding.load(vocab, 300, '../input/glove840b300dtxt/glove.840B.300d.txt')\n",
    "print('pizza', embedding_weights[vocab['pizza']][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, GRU, Dot, Concatenate\n",
    "\n",
    "def make_model():\n",
    "  # define inputs\n",
    "  image_input = Input(shape=(100352,))\n",
    "  caption_input = Input(shape=(16,))\n",
    "  noise_input = Input(shape=(16,))\n",
    "  \n",
    "  # build caption representation\n",
    "  caption_embedding = Embedding(len(vocab), 300, input_length=16, weights=[embedding_weights])\n",
    "  caption_rnn = GRU(256)\n",
    "  caption_pipeline = caption_rnn(caption_embedding(caption_input))\n",
    "  \n",
    "  # noise caption representation\n",
    "  noise_pipeline = caption_rnn(caption_embedding(noise_input))\n",
    "  \n",
    "  # image representation\n",
    "  image_dense = Dense(256, activation='tanh')\n",
    "  image_pipeline = image_dense(image_input)\n",
    "\n",
    "  # dot product between\n",
    "  positive_pair = Dot(1)([image_pipeline, caption_pipeline])\n",
    "  negative_pair = Dot(1)([image_pipeline, noise_pipeline])\n",
    "  output = Concatenate()([positive_pair, negative_pair])\n",
    "  \n",
    "  # make one model for training, and models for running the text or image \n",
    "  training_model = Model(inputs=[image_input, caption_input, noise_input], outputs=output)\n",
    "  image_model = Model(inputs=image_input, outputs=image_pipeline)\n",
    "  caption_model = Model(inputs=caption_input, outputs=caption_pipeline)\n",
    "  return training_model, image_model, caption_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "  positive = y_pred[:,0]\n",
    "  negative = y_pred[:,1]\n",
    "  return K.sum(K.maximum(0., 1. - positive + negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "  positive = y_pred[:,0]\n",
    "  negative = y_pred[:,1]\n",
    "  return K.mean(positive > negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model, image_model, caption_model = make_model()\n",
    "training_model.compile(loss=custom_loss, optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.copy(captions)\n",
    "fake_labels = np.zeros((len(features), 1))\n",
    "X_train = [features[:900], captions[:900], noise[:900]]\n",
    "Y_train = fake_labels[:900]\n",
    "X_valid = [features[-100:], captions[-100:], noise[-100:]]\n",
    "Y_valid = fake_labels[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 100352) (900, 16) (900, 1) (100, 100352) (100, 16) (100, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0].shape, X_train[1].shape, Y_train.shape, X_valid[0].shape, X_valid[1].shape, Y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 4s 253ms/step - loss: 131.0207 - accuracy: 0.4822 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 203ms/step - loss: 82.9351 - accuracy: 0.5833 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 204ms/step - loss: 71.4743 - accuracy: 0.6433 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 204ms/step - loss: 54.8844 - accuracy: 0.6889 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 199ms/step - loss: 52.5832 - accuracy: 0.7011 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 203ms/step - loss: 51.9304 - accuracy: 0.7233 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 217ms/step - loss: 53.1136 - accuracy: 0.7100 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 201ms/step - loss: 50.0273 - accuracy: 0.7111 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 202ms/step - loss: 40.0632 - accuracy: 0.7622 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 207ms/step - loss: 41.7292 - accuracy: 0.7500 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "  np.random.shuffle(noise) # donâ€™t forget to shuffle mismatched captions\n",
    "  training_model.fit(X_train, Y_train, validation_data=[X_valid, Y_valid], epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_model.save('model.image', save_format='tf')\n",
    "# caption_model.save('model.caption', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_model.save('image-text-model', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# model = tf.keras.models.load_model('../input/recipe1m-joint-image-text-keras/image-text-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Retrain on next 1000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...   91  376   27]\n",
      " [   0    0    0 ... 2330  359  113]\n",
      " [   0    0    0 ...   11 1354    3]\n",
      " ...\n",
      " [   0    0    0 ...  471  101   57]\n",
      " [   0    0    0 ...  706   62  280]\n",
      " [   0    0    0 ...  498    1 1503]]\n",
      "15/15 [==============================] - 3s 205ms/step - loss: 63.8878 - accuracy: 0.6389 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 200ms/step - loss: 63.6110 - accuracy: 0.6467 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 202ms/step - loss: 60.6103 - accuracy: 0.6367 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 202ms/step - loss: 57.4809 - accuracy: 0.6422 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 225ms/step - loss: 55.8413 - accuracy: 0.6667 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 224ms/step - loss: 55.0871 - accuracy: 0.6456 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 203ms/step - loss: 52.9241 - accuracy: 0.6578 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 205ms/step - loss: 47.8681 - accuracy: 0.6811 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 200ms/step - loss: 43.2857 - accuracy: 0.7056 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 203ms/step - loss: 38.2024 - accuracy: 0.7511 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "features, texts = load('../input/train-labels/train_label_1999.txt', '../input/train-2000-recipe-images/train_image_encoding2000.csv')\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "captions = pad_sequences(sequences, maxlen=16)\n",
    "print(captions)\n",
    "\n",
    "noise = np.copy(captions)\n",
    "fake_labels = np.zeros((len(features), 1))\n",
    "X_train = [features[:900], captions[:900], noise[:900]]\n",
    "Y_train = fake_labels[:900]\n",
    "X_valid = [features[-100:], captions[-100:], noise[-100:]]\n",
    "Y_valid = fake_labels[-100:]\n",
    "\n",
    "for epoch in range(10):\n",
    "  np.random.shuffle(noise) # donâ€™t forget to shuffle mismatched captions\n",
    "  training_model.fit(X_train, Y_train, validation_data=[X_valid, Y_valid], epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Next 1000 Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ... 1068  595  273]\n",
      " [   0    0    0 ... 4202  159  414]\n",
      " [   0    0    0 ...  798   16  275]\n",
      " ...\n",
      " [   0    0    0 ...  101    2   24]\n",
      " [   0    0    0 ...   26 2069   37]\n",
      " [   0    0    0 ... 2031  680  562]]\n",
      "15/15 [==============================] - 3s 208ms/step - loss: 54.4322 - accuracy: 0.6489 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 206ms/step - loss: 51.5811 - accuracy: 0.6444 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 200ms/step - loss: 50.3160 - accuracy: 0.6533 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 199ms/step - loss: 46.5410 - accuracy: 0.6811 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 210ms/step - loss: 41.9458 - accuracy: 0.7222 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 201ms/step - loss: 40.9910 - accuracy: 0.7211 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 205ms/step - loss: 38.5012 - accuracy: 0.7344 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 198ms/step - loss: 33.4594 - accuracy: 0.7700 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 204ms/step - loss: 31.2972 - accuracy: 0.7944 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 213ms/step - loss: 31.5035 - accuracy: 0.7911 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "features, texts = load('../input/train-labels/train_label_2999.txt', '../input/train-3000-recipe-images/train_image_encoding3000.csv')\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "captions = pad_sequences(sequences, maxlen=16)\n",
    "print(captions)\n",
    "\n",
    "noise = np.copy(captions)\n",
    "fake_labels = np.zeros((len(features), 1))\n",
    "X_train = [features[:900], captions[:900], noise[:900]]\n",
    "Y_train = fake_labels[:900]\n",
    "X_valid = [features[-100:], captions[-100:], noise[-100:]]\n",
    "Y_valid = fake_labels[-100:]\n",
    "\n",
    "for epoch in range(10):\n",
    "  np.random.shuffle(noise) # donâ€™t forget to shuffle mismatched captions\n",
    "  training_model.fit(X_train, Y_train, validation_data=[X_valid, Y_valid], epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Next 1000 Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...   76   23    4]\n",
      " [   0    0    0 ...  186   52   10]\n",
      " [   0    0    0 ...  887 2070  218]\n",
      " ...\n",
      " [   0    0    0 ...    0 4574   90]\n",
      " [   0    0    0 ... 1545 3097 1639]\n",
      " [   0    0    0 ... 4575 4576  493]]\n",
      "15/15 [==============================] - 3s 207ms/step - loss: 46.7689 - accuracy: 0.6744 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 198ms/step - loss: 41.3511 - accuracy: 0.7200 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 208ms/step - loss: 36.5540 - accuracy: 0.7633 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 206ms/step - loss: 36.8964 - accuracy: 0.7544 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 199ms/step - loss: 31.6355 - accuracy: 0.7967 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 204ms/step - loss: 34.0475 - accuracy: 0.7778 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 4s 242ms/step - loss: 30.9326 - accuracy: 0.7933 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 211ms/step - loss: 30.8404 - accuracy: 0.7989 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 207ms/step - loss: 27.0044 - accuracy: 0.8222 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 204ms/step - loss: 25.4999 - accuracy: 0.8311 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "features, texts = load('../input/train-labels/train_label_3999.txt', '../input/train-4000-recipes-images/train_image_encoding4000.csv')\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "captions = pad_sequences(sequences, maxlen=16)\n",
    "print(captions)\n",
    "\n",
    "noise = np.copy(captions)\n",
    "fake_labels = np.zeros((len(features), 1))\n",
    "X_train = [features[:900], captions[:900], noise[:900]]\n",
    "Y_train = fake_labels[:900]\n",
    "X_valid = [features[-100:], captions[-100:], noise[-100:]]\n",
    "Y_valid = fake_labels[-100:]\n",
    "\n",
    "for epoch in range(10):\n",
    "  np.random.shuffle(noise) # donâ€™t forget to shuffle mismatched captions\n",
    "  training_model.fit(X_train, Y_train, validation_data=[X_valid, Y_valid], epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Next 1000 Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...  34 175 885]\n",
      " [  0   0   0 ... 454  25   9]\n",
      " [  0   0   0 ...  41 428 179]\n",
      " ...\n",
      " [  0   0   0 ...   1 108 535]\n",
      " [  0   0   0 ... 101  30  89]\n",
      " [  0   0   0 ...   2 692   7]]\n",
      "15/15 [==============================] - 3s 222ms/step - loss: 40.4073 - accuracy: 0.7222 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 221ms/step - loss: 37.6278 - accuracy: 0.7400 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 205ms/step - loss: 36.0948 - accuracy: 0.7678 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 203ms/step - loss: 36.3567 - accuracy: 0.7578 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 200ms/step - loss: 34.4635 - accuracy: 0.7711 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 203ms/step - loss: 32.2308 - accuracy: 0.7778 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 202ms/step - loss: 30.9342 - accuracy: 0.7933 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 201ms/step - loss: 29.1329 - accuracy: 0.8078 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 204ms/step - loss: 25.2726 - accuracy: 0.8344 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 201ms/step - loss: 27.5717 - accuracy: 0.8300 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "features, texts = load('../input/train-labels/train_label_4999.txt', '../input/train-5000-recipes-images/train_image_encoding5000.csv')\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "captions = pad_sequences(sequences, maxlen=16)\n",
    "print(captions)\n",
    "\n",
    "noise = np.copy(captions)\n",
    "fake_labels = np.zeros((len(features), 1))\n",
    "X_train = [features[:900], captions[:900], noise[:900]]\n",
    "Y_train = fake_labels[:900]\n",
    "X_valid = [features[-100:], captions[-100:], noise[-100:]]\n",
    "Y_valid = fake_labels[-100:]\n",
    "\n",
    "for epoch in range(10):\n",
    "  np.random.shuffle(noise) # donâ€™t forget to shuffle mismatched captions\n",
    "  training_model.fit(X_train, Y_train, validation_data=[X_valid, Y_valid], epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...    2  272  158]\n",
      " [   0    0    0 ...  105   71    9]\n",
      " [   0    0    0 ...    0   84 4760]\n",
      " ...\n",
      " [   0    0    0 ...  149  321  227]\n",
      " [   0    0    0 ...  120   68  371]\n",
      " [   0    0    0 ...  263 1864 3043]]\n",
      "15/15 [==============================] - 3s 205ms/step - loss: 43.6037 - accuracy: 0.6856 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 213ms/step - loss: 41.7132 - accuracy: 0.7056 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 208ms/step - loss: 37.9201 - accuracy: 0.7544 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 199ms/step - loss: 34.9827 - accuracy: 0.7667 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 199ms/step - loss: 34.4214 - accuracy: 0.7700 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 204ms/step - loss: 34.6900 - accuracy: 0.7722 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 202ms/step - loss: 32.5572 - accuracy: 0.7800 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 205ms/step - loss: 30.3796 - accuracy: 0.7978 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 233ms/step - loss: 30.7087 - accuracy: 0.8011 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 210ms/step - loss: 29.4607 - accuracy: 0.7900 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "features, texts = load('../input/train-labels/train_label_5999.txt', '../input/train-6000-recipes-images/train_image_encoding6000.csv')\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "captions = pad_sequences(sequences, maxlen=16)\n",
    "print(captions)\n",
    "\n",
    "noise = np.copy(captions)\n",
    "fake_labels = np.zeros((len(features), 1))\n",
    "X_train = [features[:900], captions[:900], noise[:900]]\n",
    "Y_train = fake_labels[:900]\n",
    "X_valid = [features[-100:], captions[-100:], noise[-100:]]\n",
    "Y_valid = fake_labels[-100:]\n",
    "\n",
    "for epoch in range(10):\n",
    "  np.random.shuffle(noise) # donâ€™t forget to shuffle mismatched captions\n",
    "  training_model.fit(X_train, Y_train, validation_data=[X_valid, Y_valid], epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...    1    6 2511]\n",
      " [   0    0    0 ... 3216  166 3217]\n",
      " [   0    0    0 ...  765   92   80]\n",
      " ...\n",
      " [   0    0    0 ...    2  346 3279]\n",
      " [   0    0    0 ... 2145   17  216]\n",
      " [   0    0    0 ...  645  437  535]]\n",
      "15/15 [==============================] - 3s 200ms/step - loss: 45.5861 - accuracy: 0.6911 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 201ms/step - loss: 42.3730 - accuracy: 0.6956 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 199ms/step - loss: 33.9624 - accuracy: 0.7611 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 220ms/step - loss: 32.9729 - accuracy: 0.7822 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 218ms/step - loss: 34.8881 - accuracy: 0.7667 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 204ms/step - loss: 33.2316 - accuracy: 0.7733 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 205ms/step - loss: 31.7500 - accuracy: 0.7900 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 210ms/step - loss: 33.0177 - accuracy: 0.7889 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 200ms/step - loss: 30.6817 - accuracy: 0.7956 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 199ms/step - loss: 29.4100 - accuracy: 0.7956 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "features, texts = load('../input/train-labels/train_label_6999.txt', '../input/train-7000-10000-recipes-images/train_image_encoding7000/train_image_encoding7000.csv')\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "captions = pad_sequences(sequences, maxlen=16)\n",
    "print(captions)\n",
    "\n",
    "noise = np.copy(captions)\n",
    "fake_labels = np.zeros((len(features), 1))\n",
    "X_train = [features[:900], captions[:900], noise[:900]]\n",
    "Y_train = fake_labels[:900]\n",
    "X_valid = [features[-100:], captions[-100:], noise[-100:]]\n",
    "Y_valid = fake_labels[-100:]\n",
    "\n",
    "for epoch in range(10):\n",
    "  np.random.shuffle(noise) # donâ€™t forget to shuffle mismatched captions\n",
    "  training_model.fit(X_train, Y_train, validation_data=[X_valid, Y_valid], epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...    0 3280  491]\n",
      " [   0    0    0 ...   51  428  179]\n",
      " [   0    0    0 ...   45   15  522]\n",
      " ...\n",
      " [   0    0    0 ...   38   18   10]\n",
      " [   0    0    0 ...   26 1388  196]\n",
      " [   0    0    0 ...    6  678   33]]\n",
      "15/15 [==============================] - 3s 210ms/step - loss: 39.9388 - accuracy: 0.7356 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 203ms/step - loss: 36.1664 - accuracy: 0.7667 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 205ms/step - loss: 36.4243 - accuracy: 0.7544 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 204ms/step - loss: 35.2416 - accuracy: 0.7667 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 214ms/step - loss: 32.6066 - accuracy: 0.7811 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 204ms/step - loss: 29.7999 - accuracy: 0.7989 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 213ms/step - loss: 30.2474 - accuracy: 0.8000 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 205ms/step - loss: 26.7777 - accuracy: 0.8378 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 217ms/step - loss: 25.2010 - accuracy: 0.8333 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 4s 237ms/step - loss: 25.0904 - accuracy: 0.8311 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "features, texts = load('../input/train-labels/train_label_7999.txt', '../input/train-7000-10000-recipes-images/train_image_encoding8000/train_image_encoding8000.csv')\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "captions = pad_sequences(sequences, maxlen=16)\n",
    "print(captions)\n",
    "\n",
    "noise = np.copy(captions)\n",
    "fake_labels = np.zeros((len(features), 1))\n",
    "X_train = [features[:900], captions[:900], noise[:900]]\n",
    "Y_train = fake_labels[:900]\n",
    "X_valid = [features[-100:], captions[-100:], noise[-100:]]\n",
    "Y_valid = fake_labels[-100:]\n",
    "\n",
    "for epoch in range(10):\n",
    "  np.random.shuffle(noise) # donâ€™t forget to shuffle mismatched captions\n",
    "  training_model.fit(X_train, Y_train, validation_data=[X_valid, Y_valid], epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "9000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ... 2607 1154 1019]\n",
      " [   0    0    0 ... 5319   98  147]\n",
      " [   0    0    0 ...  927  460 3340]\n",
      " ...\n",
      " [   0    0    0 ...   26   16 1234]\n",
      " [   0    0    0 ...  246   79  446]\n",
      " [   0    0    0 ...    3  286   27]]\n",
      "15/15 [==============================] - 3s 212ms/step - loss: 38.1776 - accuracy: 0.7344 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 210ms/step - loss: 37.4517 - accuracy: 0.7422 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 4s 236ms/step - loss: 33.3627 - accuracy: 0.7778 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 222ms/step - loss: 32.9840 - accuracy: 0.7656 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 202ms/step - loss: 29.5212 - accuracy: 0.7978 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 201ms/step - loss: 29.4616 - accuracy: 0.8033 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 209ms/step - loss: 27.5480 - accuracy: 0.8244 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 207ms/step - loss: 26.5938 - accuracy: 0.8267 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 208ms/step - loss: 24.0031 - accuracy: 0.8522 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 209ms/step - loss: 22.8282 - accuracy: 0.8522 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "features, texts = load('../input/train-labels/train_label_8999.txt', '../input/train-7000-10000-recipes-images/train_image_encoding9000/train_image_encoding9000.csv')\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "captions = pad_sequences(sequences, maxlen=16)\n",
    "print(captions)\n",
    "\n",
    "noise = np.copy(captions)\n",
    "fake_labels = np.zeros((len(features), 1))\n",
    "X_train = [features[:900], captions[:900], noise[:900]]\n",
    "Y_train = fake_labels[:900]\n",
    "X_valid = [features[-100:], captions[-100:], noise[-100:]]\n",
    "Y_valid = fake_labels[-100:]\n",
    "\n",
    "for epoch in range(10):\n",
    "  np.random.shuffle(noise) # donâ€™t forget to shuffle mismatched captions\n",
    "  training_model.fit(X_train, Y_train, validation_data=[X_valid, Y_valid], epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ... 3348   50   33]\n",
      " [   0    0    0 ...   43  562   53]\n",
      " [   0    0    0 ...    5  541  485]\n",
      " ...\n",
      " [   0    0    0 ...    1  241   12]\n",
      " [   0    0    0 ...   49 5677   27]\n",
      " [   0    0    0 ...  608   19  666]]\n",
      "15/15 [==============================] - 3s 202ms/step - loss: 42.5128 - accuracy: 0.7078 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 202ms/step - loss: 38.5983 - accuracy: 0.7222 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 198ms/step - loss: 35.2306 - accuracy: 0.7622 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 203ms/step - loss: 33.1654 - accuracy: 0.7756 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 200ms/step - loss: 32.8606 - accuracy: 0.7711 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 199ms/step - loss: 31.7078 - accuracy: 0.8022 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 201ms/step - loss: 27.8103 - accuracy: 0.8211 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 200ms/step - loss: 27.5566 - accuracy: 0.8067 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 4s 248ms/step - loss: 27.6638 - accuracy: 0.8144 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "15/15 [==============================] - 3s 205ms/step - loss: 26.0880 - accuracy: 0.8311 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "features, texts = load('../input/train-labels/train_label_9999.txt', '../input/train-7000-10000-recipes-images/train_image_encoding10000/train_image_encoding10000.csv')\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "captions = pad_sequences(sequences, maxlen=16)\n",
    "print(captions)\n",
    "\n",
    "noise = np.copy(captions)\n",
    "fake_labels = np.zeros((len(features), 1))\n",
    "X_train = [features[:900], captions[:900], noise[:900]]\n",
    "Y_train = fake_labels[:900]\n",
    "X_valid = [features[-100:], captions[-100:], noise[-100:]]\n",
    "Y_valid = fake_labels[-100:]\n",
    "\n",
    "for epoch in range(10):\n",
    "  np.random.shuffle(noise) # donâ€™t forget to shuffle mismatched captions\n",
    "  training_model.fit(X_train, Y_train, validation_data=[X_valid, Y_valid], epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model.save('10000-model.image')\n",
    "caption_model.save('10000-model.caption')\n",
    "training_model.save('10000-image-text-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow import keras\n",
    "# reconstructed_model = keras.models.load_model(\"../input/recipe1m-joint-image-text-keras/image-text-model\", compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Test on 1000 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.load('../input/train-1000-recipe-images/test_image_encoding_1000.npy')\n",
    "texts = []\n",
    "with open('../input/test-labels-recipe1m/test_label_999.txt') as fp:\n",
    "    for line in fp:\n",
    "      tokens = line.strip().split()\n",
    "      texts.append(' '.join(tokens[1:]))\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "captions = pad_sequences(sequences, maxlen=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_representations = caption_model.predict(captions)\n",
    "image_representations = image_model.predict(features)\n",
    "np.save('caption-test-1000-representations', caption_representations)\n",
    "np.save('image-test-1000-representations', image_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94773248/94765736 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "resnet_model = ResNet50(weights='imagenet', include_top=False)\n",
    "\n",
    "def extract_features(img_path): # though feature vector for training images has been precalculated and present in a file still function is needed for unseen images\n",
    "  img = image.load_img(img_path, target_size=(224, 224))\n",
    "  x = image.img_to_array(img)\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "  x = preprocess_input(x)\n",
    "  features = resnet_model.predict(x)\n",
    "  return np.expand_dims(features.flatten(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image_filename, n=10):\n",
    "  # generate image representation for new image\n",
    "  image_representation = image_model.predict(extract_features(image_filename))\n",
    "  # compute score of all captions in the dataset\n",
    "  scores = np.dot(caption_representations, image_representation.T).flatten()\n",
    "  # compute indices of n best captions\n",
    "  indices = np.argpartition(scores, -n)[-n:]\n",
    "  indices = indices[np.argsort(scores[indices])]\n",
    "  # display them\n",
    "  for i in [int(x) for x in reversed(indices)]:\n",
    "    print(scores[i], texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6873505 Molasses Raisin Bread\n",
      "0.20260657 Easy Chocolate-Caramel Brownies\n",
      "0.16035119 Chocolate Brown-Sugar Brownies\n",
      "0.1024929 Strawberry Nut Bread\n",
      "0.07332119 Garlic Bread (Pane Strofinato All'Aglio)\n",
      "0.0057524666 Hershey's Low Fat Banana Bread\n",
      "0.002683401 Peanut Butter Crumb Topped Brownies\n",
      "-0.038095728 Cheese and Salami Loaf\n",
      "-0.04550138 Hearth Bread\n",
      "-0.055847332 Delicious Pumpkin Bread\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-db0e0bf2d500>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgenerate_caption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/recipe1m-test-jpg1/1bbb06bc58.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/recipe1m-test-jpg1/1bbb06bc58.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "generate_caption('../input/recipe1m-test-jpg1/1bbb06bc58.jpg')\n",
    "Image('../input/recipe1m-test-jpg1/1bbb06bc58.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# def preprocess_texts(texts):\n",
    "#   output = []\n",
    "#   for text in texts:\n",
    "#     output.append([vocab[word] if word in vocab else 0 for word in text.split()])\n",
    "#   return pad_sequences(output, maxlen=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def search_image(caption, n=10):\n",
    "#   caption_representation = caption_model.predict(preprocess_texts([caption]))\n",
    "#   scores = np.dot(image_representations, caption_representation.T).flatten()\n",
    "#   indices = np.argpartition(scores, -n)[-n:]\n",
    "#   indices = indices[np.argsort(scores[indices])]\n",
    "#   for i in [int(x) for x in reversed(indices)]:\n",
    "#     print(scores[i], images[i])\n",
    "#     display(Image('05-caption-images/' + images[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# search_image('kids playing football', 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
